
<html lang="en">
<head>
    {% include header.html %}
    
</head>

<body>

<script src="../js/holder.js"></script>


<div id="nav-placeholder">{% include navbar.html %}</div>


<div id="ds-wikilinks-top" style="margin-top:-60px"></div>
<div style="margin-top:60px"></div>

<div class="container">
    <section class="row">
        <h2 class="page-header span10 offset1"><a href="/datasets" style="color:#881c1c">Datasets</a> ></h2>
        <div class="span10 offset1">
            <div>
                <h3>Wikilinks Dataset</h3>
            </div>
            <!-- <div style="clear:both"></div> -->
            <p class="desc">
                Cross-document coreference resolution is the task of grouping the entity mentions in a collection of documents into sets that each represent a distinct entity. It is central to knowledge base construction and also useful for joint inference with other NLP components. Obtaining large, organic labeled datasets for training and testing cross-document coreference has previously been difficult. We use a method for automatically gathering massive amounts of naturally-occurring cross-document reference data to create the Wikilinks dataset comprising of 40 million mentions over 3 million entities. Our method is based on finding hyperlinks to Wikipedia from a web crawl and using anchor text as mentions. In addition to providing large-scale labeled data without human effort, we are able to include many styles of text beyond newswire and many entity types beyond people.
            </p>
        </div>
    </section>

    <section class="row">
        <div class="span4 offset1">
                <p style="text-align:center;"><img src="./data/wikilinks-figure.jpg" style="margin-top: 0px; width:100%;"/></p>
        </div>

        <div class="span6">
            <p class="desc">
                Manually labeling mentions with the entities they refer to is time-consuming and expensive annotation task. However, page authors often use hyperlinks to annotate some of the mentions in their texts with extra information. If these links are to Wikipedia pages, then the link targets can be used to disambiguate the mentions from each other. For example, consider the two mentions of "Banksy" from different webpages (as shown in the figure). Since both links point to the same Wikipedia entity, it is likely that the two mentions refer to the same entity. Using this approach, we are able to construct an automatically annotated dataset of containing the mentions extracted from the web and the entities in Wikipedia that they refer to.
            </p>
        </div>
    </section>

    <section class="row">
        <div class="span10 offset1">

            <div>
                <h3 style="color:grey">Statistics</h3>
            </div>
            <p class="desc">
                <ul style="padding-left: 15px">
                    <li>Number of Mentions: 40,323,863</li>
                    <li>Number of Entities:  2,933,659</li>
                    <li>Number of pages:    10,893,248</li>
                </ul>
            </p>

            <div>
                <h3 style="color:grey">Citation</h3>
            </div>
            <p class="desc">
                The dataset has been described in the following technical report: <a href="https://web.cs.umass.edu/publication/docs/2012/UM-CS-2012-015.pdf">Wikilinks: A Large-scale Cross-Document Coreference Corpus Labeled via Links to Wikipedia</a>
            </p>

            <p class="desc">
                If you use the dataset, please use the following citation:
            </p>

            <!-- <div class="sites-codeblock sites-codesnippet-block">
                <div><code>@techreport{singh12:wiki-links,</code></div>
                <div><code>      author    = "Sameer Singh and Amarnag Subramanya and Fernando Pereira and Andrew McCallum",</code></div>
                <div><code>      title     = "Wikilinks: A Large-scale Cross-Document Coreference Corpus Labeled via Links to {Wikipedia}",</code></div>
                <div><code>      institute = "University of Massachusetts, Amherst",</code></div>
                <div><code>      number    = "UM-CS-2012-015",</code></div>
                <div><code>      year      = "2012"</code></div>
                <div><code>}</code></div>
            </div> -->
            <!-- <div class="sites-codeblock sites-codesnippet-block"> -->
<pre><code>@techreport{singh12:wiki-links,
    author    = "Sameer Singh and Amarnag Subramanya and Fernando Pereira and Andrew McCallum",
    title     = "Wikilinks: A Large-scale Cross-Document Coreference Corpus Labeled via Links to {Wikipedia}",
    institute = "University of Massachusetts, Amherst",
    number    = "UM-CS-2012-015",
    year      = "2012"
}</code></pre>
            <!-- </div> -->

            <div>
                <h3 style="color:grey">Original Dataset</h3>
            </div>
            <p class="desc">
                As provided by Google, this dataset provides URLs of webpages, along with the anchor of the links, and the Wikipedia pages they link to. As provided, this dataset can be used to get all the surface strings that refer to a Wikipedia page, but further, it can be used to download the webpages and extract the context around the webpages (see below).
            </p>

            <p class="desc">
                <strong>Download the dataset here:</strong> <a href="http://code.google.com/p/wiki-links/downloads/list">http://code.google.com/p/wiki-links/downloads/list</a><br />
                You can directly download all the files using the following command (bash):
                <pre><code>for (( i=0; i<10; i++ )) do echo "Downloading file $i of 10"; wget https://wiki-links.googlecode.com/files/data-0000$i-of-00010.gz ; done</code></pre>
            </p>

            <div>
                <h3 style="color:grey">Expanded Dataset</h3>
            </div>
            <p class="desc">
                UMass has created expanded versions of the dataset containing the following extra features:
                <ul style="padding-left: 15px">
                    <li>Complete webpage content (with cleaned DOM structure)</li>
                    <li>Extracted context for the mentions</li>
                    <li>Alignment to <a href="http://freebase.com/">Freebase</a> entities</li>
                    <li>and more...</li>
                </ul>
                For description of the data format, and instructions on how to use it in any language (Scala code included), see the documentation available <a href="https://code.google.com/archive/p/wiki-link/wikis/ExpandedDataset.wiki">here</a>.
            </p>
            <p class="desc">
                We also describe the steps used to create the dataset <a href="https://code.google.com/archive/p/wiki-link/">here</a>.
            </p>
            <p class="desc">
                The expanded dataset is available in different versions. 
            </p>
            
            <div style="margin-top: 10px">
                <h4 style="color:darkgrey">Dataset with Context</h4>
            </div>
            <p class="desc">
                The smallest version that contains the context and the freebase id is ~5GB in size. You can directly download the files from <a href="http://iesl.cs.umass.edu/downloads/wiki-link/context-only/">http://iesl.cs.umass.edu/downloads/wiki-link/context-only/</a> or use the following script (run in an empty directory):
                <pre><code>for (( i=1; i<110; i++)) do echo "Downloading file $i of 109"; f=`printf "%03d" $i` ; wget http://iesl.cs.umass.edu/downloads/wiki-link/context-only/$f.gz ; done ; echo "Downloaded all files, verifying MD5 checksums (might take some time)" ; diff --brief <(wget -q -O - http://iesl.cs.umass.edu/downloads/wiki-link/context-only/md5sum) <(md5sum *.gz) ; if [ $? -eq 1 ] ; then echo "ERROR: Download incorrect\!" ; else echo "Download correct" ; fi</code></pre>
                These data files follow the thrift format for the expanded dataset, with PageContentItem fields emptied out.
            </p>

            <div style="margin-top: 10px">
                <h4 style="color:darkgrey">Dataset with Complete Webpages</h4>
            </div>
            <p class="desc">
                This version of the dataset contains the context, freebase ids, raw file, cleaned DOM structure, and the full article text. Hence, the overall size of this dataset is ~180GB.
            </p>
            <p class="desc">
                The dataset is available here: <a href="http://iesl.cs.umass.edu/downloads/wiki-link/full-content/part1/">http://iesl.cs.umass.edu/downloads/wiki-link/full-content/part1/</a> and <a href="http://iesl.cs.umass.edu/downloads/wiki-link/full-content/part2/">http://iesl.cs.umass.edu/downloads/wiki-link/full-content/part2/</a>. It is in the same format as above, with accompanying md5sum files.
            </p>

            <div>
                <h3 style="color:grey">People</h3>
            </div>
            <p class="desc">
                This dataset was created at Google, partly by <a href="http://research.google.com/pubs/author38552.html">Amarnag Subramanya</a>, <a href="https://research.google.com/pubs/author1092.html">Fernando Pereira</a>, <a href="http://sameersingh.org/">Sameer Singh</a> and <a href="https://people.cs.umass.edu/~mccallum/">Andrew McCallum</a>. We would also like to thank <a href="http://brianmartin.github.io/">Brian Martin</a>, <a href="https://harshalgem.wordpress.com/">Harshal Pandya</a>, and John R. Frank for contributing to the code that downloads, processes, and analyzes the data. This page is currently maintained by <a href="http://sameersingh.org/">Sameer Singh</a>. 
            </p>

            <div>
                <h3 style="color:grey">Relevant Publications</h3>
            </div>
            <p class="desc">
                List of papers that describe the dataset, or use it (or a variation) for evaluation.
                <ul style="padding-left: 15px">
                    <li>
                        S. Singh, A. Subramanya, F. Pereira, A. McCallum
                    </li>
                    
                    <li class="li-desc">
                    <strong>Wikilinks: A Large-scale Cross-Document Coreference Corpus Labeled via Links to Wikipedia</strong></li>
                    <li class="li-desc">
                    University of Massachusetts Amherst, CMPSCI Technical Report, UM-CS-2012-015, 2012 </li>
                    <li class="li-desc">
                    <a href="https://web.cs.umass.edu/publication/docs/2012/UM-CS-2012-015.pdf">PDF</a></li>
                        
                        
                    <li>
                        S. Singh, A. Subramanya, F. Pereira, A. McCallum
                    </li>
                        <li class="li-desc">
                        <strong>Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</strong></li>
                        <li class="li-desc">
                        Association for Computational Linguistics: Human Language Technologies (ACL HLT), 2011 </li>
                        <li class="li-desc">
                        <a href="http://www.aclweb.org/anthology/P11-1080">PDF</a></li>
                    </li>
                </ul>
                Let us know if you've published a paper using the dataset, and would like to include it here.
            </p>


            <p class="desc">
                
            </p>
        
        </div>
    </section>

<hr>

<footer id="footer-placeholder">{% include footer.html %}</footer>

</div>

<!-- /container -->
<script src="../js/jquery-latest.js"></script>
<script src="../js/bootstrap.js"></script>
<script>

    /*
    $(function() {
    $('.navbar li a').click(function(event) {
    var a = $(this)
    $('.navbar li').removeClass("active")
    a.parent().addClass("active")
    });

    })
    */

</script>



</body></html>
