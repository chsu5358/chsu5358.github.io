
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <title>UMass IESL</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    <link href="./css/iesl.css" rel="stylesheet">
    <!-- <link href="./css/pub.css" rel="stylesheet"> -->
    <link href="./css/bootstrap.css" rel="stylesheet">
    <style>
        body {
        padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
        }
    </style>
    <link href="./css/bootstrap-responsive.css" rel="stylesheet">

    <link rel="shortcut icon" href="./images/logo-iesl-32x32-ico.ico" />
<!-- CY: to be modified, purpose? -->
<style id="holderjs-style" type="text/css">.holderjs-fluid {font-size:16px;font-weight:bold;text-align:center;font-family:sans-serif;margin:0}</style><script type="text/javascript" charset="utf-8" async="" src="./js/timeline.a3df0b22c7410afb111fd9e6736adf22.js"></script></head>

<!--<body data-spy="scroll" data-target=".navbar">-->
<body>
	
<!-- CY: to check purpose? -->
<script id="twitter-wjs" src="./js/widgets.js"></script><script src="./js/holder.js"></script>


<div class="navbar navbar-fixed-top">
    <div class="navbar-inner" style="margin:0 auto">
        <div class="container" style="margin:0 auto">
            <div class="navbar-header">
                <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </a>
            </div>
	    
            <div class="nav-collapse collapse">
                <ul class="nav" style="text-align: center; float:none; margin: auto">
                    <li class="pull-left" style="float:none; display: inline-block;"><a class="brand" href="https://chsu5358.github.io">UMass IESL</a></li>
                    <!--<li class="active"><a href="#">Home</a></li>-->
            <!-- CY:hosted on https://chsu5358.github.io/ -->
                    <!-- <li style="float:none; display: inline-block;"><a href="#news" class="smoothScroll">News</a></li> -->
                    <li><a href="https://chsu5358.github.io/news.html">News</a></li>
                    <li><a href="https://chsu5358.github.io/people.html">People</a></li>
                    <li class="active"><a href="https://chsu5358.github.io/research.html">Research</a></li>
                    <li><a href="https://chsu5358.github.io/projects.html">Projects</a></li>
                    <li><a href="https://chsu5358.github.io/pub.html">Publications</a></li>
                    <li><a href="https://chsu5358.github.io/software.html">Software</a></li>
                    <li><a href="https://chsu5358.github.io/datasets.html">Datasets</a></li>
                    <li><a href="https://chsu5358.github.io/social.html">Social</a></li>
                    <li><a href="https://sites.google.com/a/iesl.cs.umass.edu/wiki/">Wiki (internal)</a></li>

                    <!-- CY: to figure out: format, search action
                    <li>
                        <form id="demo-2">
                            <input type="search" placeholder="Search our site">
                        </form>
                    </li>
                    -->
                </ul>

                <!-- CY: search -->
                <!-- <form class="navbar-form navbar-left">
                    <div class="input-group">
                    <input type="text" class="form-control" placeholder="Search">
                        <div class="input-group-btn">
                            <button class="btn btn-default" type="submit">
                                <i class="glyphicon glyphicon-search"></i>
                            </button>
                        </div>
                    </div>
                </form> -->
            </div>
            <!--/.nav-collapse -->
        </div>
    </div>
</div>


<div id="research-top" style="margin-top:-60px"></div>
<div style="margin-top:60px"></div>

<div class="container">
    <section class="row">
        <h2 id="research-header" class="page-header span10 offset1">Research</h2>

        <div class="span10 offset1">
            <div>
                <h3>Unified Information Extraction and Data Mining</h3>
            </div>

            <p class="desc">
                Although information extraction and data mining appear together in many applications, their interface in most current deployments would better be described as serial juxtaposition than as tight integration. Information extraction populates slots in a database by identifying relevant subsequences of text, but is usually not aware of the emerging patterns and regularities in the database. Data mining methods begin from a populated database, and are often unaware of where the data came from, or its inherent uncertainties. The result is that the accuracy of both suffers, and significant mining of complex text sources is beyond reach.
            </p>

            <p class="desc">
                We have been researching relational probabilistic models that <a href="http://www.cs.umass.edu/~mccallum/papers/iedatamining-ijcaiws03.pdf">unify extraction and mining</a>, so that by sharing common inference procedures, they can each overcome the weaknesses of the other. For example, data mining run on a partially-filled database can find patterns that provide "top-down" accuracy-improving constraints to information extraction. Information extraction can provide a much richer set of "bottom-up" hypotheses to data mining if the mining is able to handle additional uncertainty information from extraction.
            </p>
        </div>

        <div class="span10 offset1">
            <div>
                <h3>Conditional Random Fields for Relational Data, Approximate Inference and Learning</h3>
            </div>

            <p class="desc">
                The above unified processing requires large-scala joint inference that cannot be performed exactly. We have been developing various methods of MCMC inference methods and corresponding learning approaches aimed specifically at extremely large relational-data domains. Our approach based on Metropolis-Hastings inference and learning by ranking, achieved <a href="http://www.cs.umass.edu/~mccallum/papers/coref-hlt2007.pdf">best-in-the-world coreference resolution</a> on a standard newswire dataset. This work is also quite relevant to recent interest in "combining logic and probability".
            </p>
        </div>

        <div class="span10 offset1">
            <div>
                <h3>Automatic Knowledge Base Construction, with Probabilistic Databases & Human Cooperation</h3>
            </div>

            <p class="desc">
                Incorporating uncertainty and probabilistic inference into databases has posed many challenges, often leading to systems that sacrifice modeling power, scalability, or restrict the class of relational algebra formulae under which they are closed.  We have proposed [Wick et al, VLDB, 2009] an alternative approach in which the underlying relational database always represents a single possible world, and an external, unrestricted factor graph encodes a distribution over the set of possible worlds; Markov chain Monte Carlo (MCMC) inference is then used to recover this uncertainty to a desired level of fidelity.  Our approach allows the efficient evaluation of arbitrary SQL queries over probabilistic databases with dependencies expressed by unrestricted graphical models, (including graphical model structure that changes during inference).
            </p>

            <p class="desc">
                As a result, state-of-the-art, machine-learning-based information extraction, entity resolution, schema-aligment can then be efficiently run "inside" the database.  Furthermore, rather than running in pipeline fashion, they can all interoperate in the same scalable infrastructure, imparting the increased accuracy of joint inference. This also provides a convenient any-time functionality to KB construction and query processing.
            </p>

            <p class="desc">
                In addition, we advocate an approach to information integration (including knowledge-base construction by human-machine cooperation) in which the the canonical "true" entities and relations in the database are always inferred from integrated or human-entered data, never injected directly.  Even human "corrections" to the KB are merely additional pieces of evidence input as probabilistic evidence---allowing our system to reason probabilistically about provenance and trust.
            </p>

            <p class="desc">
                MCMC sampling provides efficiency by hypothesizing modifications to possible worlds rather than generating entire worlds from scratch.  Queries are then run over the portions of the world that change, avoiding the onerous cost of running full queries over each sampled world.  We leverage materialzed view maintenance techniques to dramatically speed query processing.  We demonstrate systemâ€™s ability to answer relational queries with aggregation over one million tuples, and show additional scalability through use of parallelization.
            </p>

        </div>

        <div class="span10 offset1">
            <div>
                <h3>Extraction, Integration and Mining of Bibliographic Data</h3>
            </div>

            <p class="desc">
                Back in the 1997 I conceived of and lead a project at JustResearch that created <a href="http://www.cora.whizbang.com/"><i>Cora</i></a>, one of the first domain-specific search engines over computer science research papers. You can read more about our research on Cora in our IRJ <a href="http://www-2.cs.cmu.edu/~mccallum/papers/cora-irj2000.ps.gz">journal paper</a> or a <a href="https://people.cs.umass.edu/~mccallum/papers/cora-aaaiss99.ps.gz">paper</a> presented at the <a href="http://www.aaai.org/Symposia/Spring/1999/sssregistration-99.html">AAAI'99 Spring Symposium</a>. The Cora team also included <a href="http://www.cs.cmu.edu/%7Eknigam">Kamal Nigam</a>, <a href="http://www.cs.cmu.edu/%7Ekseymore">Kristie Seymore</a>, <a href="http://www.ai.mit.edu/%7Ejrennie">Jason Rennie</a>, <a href="http://www.informatik.uni-trier.de/%7Eley/db/indices/a-tree/c/Chang:Huan.html">Huan Chang</a> and <a href="http://www-2.cs.cmu.edu/%7Ejcreed/">Jason Reed</a>.
            </p>

            <p class="desc">
                More recently we have been working on an enhanced alternative to <a href="http://scholar.google.com">Google Scholar</a>, <a href="http://citeseer.ist.psu.edu/">CiteSeer</a>, and other digital libraries of the research literature. Our system, called <a href="http://rexa.info"><em>Rexa</em></a>, automatically extracts a de-duplicated cross-referenced database of not just papers (and references), but also people, grants, publication venues and institutions. We also perform various kinds of <a href="http://www.cs.umass.edu/%7Emccallum/papers/pam-icml06.pdf">topic</a> and <a href="http://www.cs.umass.edu/%7Emccallum/papers/impact-jcdl06.pdf">bibliometric impact</a> analysis on this data.
            </p>
        </div>

        <div class="span10 offset1">
            <div>
                <h3>Social Network Analysis with Structured Topic Models</h3>
            </div>

            <p class="desc">
                Traditional social network analysis examines the connectivity of entities in a graph. However, in many cases we have data not just about the existence of a graph-edge, but also various properties of the nodes and edges---including large quantities of corresponding textual data. We have used Bayesian latent variable models, variants of "topic models" augmented with non-textual variables to (a) <a href="http://www.cs.umass.edu/%7Emccallum/papers/art-jair07.pdf">discover roles</a> of people in the sender-receiver structure of a large email collection, (b) <a href="http://www.cs.umass.edu/%7Emccallum/papers/gt-bookch07.pdf">discover groups</a> (coalitions) of U.S. senators or U.N. countries from their voting records and the topics of the bills, (c) <a href="http://www.cs.ubc.ca/%7Emurphyk/nips07NetworkWorkshop/abstracts/mimno.pdf">discover communities</a> of academic researchers from their papers and the venues in which they publish.
            </p>
        </div>

        <div class="span10 offset1">
            <div>
                <h3>Semi-supervised Learning & Alignment Learning in Natural Language</h3>
            </div>

            <p class="desc">
                The only way to put natural language learning into the hands of the people is to reduce the burden of labeling training data. Over the years we have worked on various methods of semi-supervised learning that combines small amounts of labeled data with large amounts of unlabeled data. Our most recent work is in <a href="http://www.cs.umass.edu/%7Emccallum/papers/ge08note.pdf">Generalized Expectation</a> (GE) criteria, one form of which can be understood as enabling "<a href="http://www.cs.umass.edu/%7Emccallum/papers/druck08sigir.pdf">feature labeling</a>" as opposed to the traditional "instance labeling".
            </p>

            <p class="desc">
                We have also removed the need for human labeled data entirely by leveraging information already in relevant databases, and learning information extractors by discovering <a href="http://www.cs.umass.edu/%7Emccallum/papers/crfstredit-uai05.pdf">CRF-based alignments</a> between <a href="http://www.cs.umass.edu/%7Emccallum/papers/bellare-iiweb07.pdf">database records and unstructured text</a>.
            </p>
        </div>

        <div class="span10 offset1">
            <div>
                <h3>Joint Inference for NLP, Dialog Pragmatics, Perception and Action</h3>
            </div>

            <p class="desc">
                As part of a MURI project joint with UPenn, we have begun work on probabilistic modeling of natural language dialog, designing methods that will do joint, unified inference all the way from natural language understanding, through dialog pragmatics, to perception and action in a shared world. This work will leverage our research in large-scale joint inference in CRFs.
            </p>
        </div>

        <div class="span10 offset1">
            <div>
                <h3>Intelligent Understanding of our Email World</h3>
            </div>

            <p class="desc">
                <p>As part of the <a href="http://www.ai.sri.com/project/CALO">CALO</a> project, we extracted information about people and other entities appearing in email streams, performed large-scale entity resolution, and discovered topics and expertise.
            </p>
        </div>

        <div class="span10 offset1">
            <div>
                <h3>Conditional Probability Models for Sequences and other Relational Data</h3>
            </div>

            <p class="desc">
                BBack in the 1990's, after having <a href="papers/ieshrink-aaaiws99.pdf">some</a> <a href="papers/iehill-aaai2000s.ps">success</a> using hidden Markov models for information extraction, we found ourselves frustrated by their lack of ability to incorporate many arbitrary, overlapping features of the input sequence, such as capitalization, lexicon memberships, spelling features, and conjunctions of such features in a large window of past and future observations. The same difficulties with non-independent features exist in many generatively-trained models historically used in NLP. We have begun work with conditionally-trained probability models that address these problems. <a href="papers/memm-icml200.ps">Maximum entropy Markov models</a> are locally-normalized conditional sequence models. Finite-state <a href="papers/crf-icml01.ps">Conditional Random Fields</a> (CRFs) are globally-normalized models. We have also been working with CRFs for <a href="papers/condid-ijcaiws2003.pdf">coreference</a> and <a href="papers/dcrf-nips03.pdf">multi-sequence labeling</a> , analogous to conditionally-trained Dynamic Bayesian Networks (DBNs). We now work with even more complex CRFs, that integrate logic and probability, as described above.
            </p>
        </div>

        <div class="span10 offset1">
            <div>
                <h3>WhizBang Labs</h3>
            </div>

            <p class="desc">
                From 2000 through 2002 I was Vice President of Research and Development at <a href="http://www.infotoday.com/newsbreaks/nb020603-3.htm">WhizBang Labs</a>, a start-up company focusing on information extraction from the Web. We developed sophisticated machine learning extraction systems for numerous application domains---among them <a href="http://www.flipdog.com">FlipDog.com</a>, a database of job openings extracted directly from company Web sites (now owned by <a href="http://www.monster.com">Monster.com</a>), corporate information for <a href="http://www.dnb.com">Dun &amp; Bradstreet</a> and <a href="http://www.lexisnexis.com/">Lexis Nexis</a>, and course syllabi for the <a href="http://www.dol.gov/">U.S. Department of Labor</a>.
            </p>
        </div>

        <div class="span10 offset1">
            <div>
                <h3>WebKB</h3>
            </div>

            <p class="desc">
                In 1996 and 1997 I was part of <a href="http://www.cs.cmu.edu/%7Etom">Tom Mitchell</a>'s <a href="http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb/">WebKB</a> project and the <a href="http://www.cs.cmu.edu/afs/cs/project/theo-4/text-learning/www/index.html">CMU Text Learning group</a> Text Learning group.
            </p>
        </div>

        <div class="span10 offset1">
            <div>
                <h3>Reinforcement Learning</h3>
            </div>

            <p class="desc">
                In what now seems like a lifetime ago, I was interested in reinforcement learning---especially with hidden state and factored representations. My <a href="http://www.cs.rochester.edu/u/mccallum/phd-thesis">thesis</a> uses memory-based learning and a robust statistical test on reward in order to learn a structured policy representation that makes perceptual and memory distinctions only where needed for the task at hand. It can also be understood as a method of <i>Value Function Approximation</i>. The model learned is an <i>order-n partially observable Markov decision process</i>. It handles noisy observation, action and reward.
            </p>

            <p class="desc">
                It is related to <a href="http://theory.lcs.mit.edu/%7Edanar">Ron</a>, <a href="http://www.research.att.com/%7Esinger">Singer</a> and <a href="http://www.cs.huji.ac.il/%7Etishby/">Tishby</a>'s <i>Probabilistic Suffix Trees</i>, <a href="http://www.cs.brown.edu/%7Elpk">Leslie Kaelbling</a>'s <i>G-algorithm</i> and <a href="http://www.cs.cmu.edu/%7Eawm">Andrew Moore</a>'s <i>Parti-game</i>. It is distinguished from similar-era work by <a href="http://www.cs.duke.edu/%7Emlittman">Michael Littman</a>, <a href="http://www.cs.ubc.ca/spider/cebly/craig.html">Craig Boutilier</a> and others in that it learns both a model and a policy, and is quite practical with infinite-horizon tasks and large state and observation spaces. Follow-on or comparison work has been done by <a href="www-anw.cs.umass.edu/%7Eajonsson">Anders Jonsson</a>, <a href="envy.cs.umass.edu/People/barto/barto.html">Andy Barto</a>, <a href="www.cs.cmu.edu/%7Ewill">Will Uther</a>, <a href="http://www.ai.mit.edu/people/nhg">Natalia Hernandez</a>, <a href="http://www.ai.mit.edu/people/lpk">Leslie Kaelbling</a>, and <a href="http://www.cs.umass.edu/%7Emahadeva/">Sridhar Mahadevan</a>.
            </p>

            <p class="desc">
                The algorithm, called <i>U-Tree</i>, was demonstrated solving a highway driving task using simulated eye-movements and deictic representations. The simulated environment has about 21000 states, 2500 observations, noise and much hidden state. After about 2 1/2 hours of simulated experience, <i>U-Tree</i> learns a task-specific model of the environment that has only 143 states. It's learned behavior included lane changes to avoid slow vehicles in front, and checking the rear-view mirror to avoid faster vehicles from behind.
            </p>
        </div>

    </section>
    

<hr>
<footer>
    <p>Â© UMass IESL 2017 </p>
    <p style="margin-top:-1.0em">
    <a href="http://www.cics.umass.edu/">College of Information and Computer Sciences</a>, <a href="http://www.umass.edu/">University of Massachusetts Amherst</a>.
    </p>
</footer>

</div>
<!-- /container -->

<script src="./js/jquery-latest.js"></script>
<script src="./js/bootstrap.js"></script>

<!-- CY: add smoothScroll -->
<script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
<script type="text/javascript" src="./js/smoothscroll.js"></script>

<!-- CY: add active class toggle in navbar -->
<script>
    $(".nav li").on("click", function() {
      $(".nav li").removeClass("active");
      $(this).addClass("active");
    });

</script>

<!-- CY: not used?
<iframe id="rufous-sandbox" scrolling="no" frameborder="0" allowtransparency="true" allowfullscreen="true" style="position: absolute; visibility: hidden; display: none; width: 0px; height: 0px; padding: 0px; border: none;" title="Twitter analytics iframe" src="./UCL Machine Reading_files/saved_resource(1).html"></iframe>
-->

</body>
</html>
